# AUTOGENERATED! DO NOT EDIT! File to edit: 01_getdata.ipynb (unless otherwise specified).

__all__ = ['url', 'headers', 'dateof', 'getweb', 'saveprovice', 'savecity']

# Cell
from bs4 import BeautifulSoup
from parser import * #regex_parser
import re
import json
import time
import logging
import datetime
import requests
import pprint

# Cell
url = "https://ncov.dxy.cn/ncovh5/view/pneumonia?from=singlemessage&isappinstalled=0"
headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.117 Safari/537.36'
}

#保存的文件名。
dateof = '20200207'

# Cell

def getweb():
    session = requests.session()
    session.headers.update(headers)
    r = session.get(url)
    soup = BeautifulSoup(r.content, 'lxml')

    #获取省市数据。
    area_information = re.search(r'\[(.*)\]', str(soup.find('script', attrs={'id': 'getAreaStat'})))
    area = json.loads(area_information.group(0))
    return area

# Cell
#写入文件，分省数据。
def saveprovice(area):
    fprovince = "data/" + "prov_" + dateof + ".csv"
    fp = open(fprovince, "w")
    fp.write("省份,确诊,疑似,治愈,死亡\r")
    for a in area:
        fp.write(a['provinceName']+','+ \
                 str(a['confirmedCount'])+','+ \
                 str(a['suspectedCount'])+','+ \
                 str(a['curedCount'])+','+ \
                 str(a['deadCount'])+ '\r')
    fp.close()
    print("writed to "+ fprovince + "\r\n")

# Cell

#写入文件，分市数据。
def savecity(area):
    fcity = "data/" + "city_" + dateof + ".csv"
    fc = open(fcity, "w")
    fc.write("省份,城市,确诊,疑似,治愈,死亡\r")
    for p in area:
        cities = p['cities']
        for c in cities:
            fc.write(p['provinceName']+','+ \
                     c['cityName']+','+ \
                     str(c['confirmedCount'])+','+ \
                     str(c['suspectedCount'])+','+ \
                     str(c['curedCount'])+','+ \
                     str(c['deadCount'])+'\r')
    fc.close()
    print("writed to "+ fcity + "\r\n")